{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Quite Plug and Chug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 3) (999, 1) (2999, 3) (2999, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split test and train data into X and y\n",
    "train_file = \"training.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "train_x = df_train.drop(df_train.columns[3], axis=1)\n",
    "train_y = df_train.drop(df_train.columns[0:3], axis=1)\n",
    "test_x = df_test.drop(df_test.columns[3], axis=1)\n",
    "test_y = df_test.drop(df_test.columns[0:3], axis=1)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdash/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08908909  0.08908909  0.08908909  0.08908909]\n",
      " [ 0.08908909  0.08908909  0.08908909  0.08908909]\n",
      " [ 0.08908909  0.08908909  0.08908909  0.08908909]\n",
      " [ 0.08908909  0.08908909  0.08908909  0.08908909]\n",
      " [ 0.02002002  0.02002002  0.02002002  0.02002002]]\n",
      "[[ 0.44148049  0.44148049  0.44148049  0.44148049]\n",
      " [ 0.44148049  0.44148049  0.44148049  0.44148049]\n",
      " [ 0.44148049  0.44148049  0.44148049  0.44148049]\n",
      " [ 0.44148049  0.44148049  0.44148049  0.44148049]\n",
      " [ 0.44114705  0.44114705  0.44114705  0.44114705]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM0AAAD8CAYAAADUmiBhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogFCRAOEiAYIEQ0QIhogRDRA6N8rBsikvs3hOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1143bd5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM0AAAD8CAYAAADUmiBhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogFCRAOEiAYIEQ0QIhogRDRA6N8rBsikvs3hOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bee080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "d = [1, 2, 3, 10]\n",
    "error_test = np.zeros((5, 4))\n",
    "error_train = np.zeros((5, 4))\n",
    "\n",
    "for i in range(len(C)):\n",
    "    for j in range(len(d)):\n",
    "        svm = SVC(C=C[i], degree=d[j])\n",
    "        svm.fit(train_x, train_y)\n",
    "        error_train[i][j] = 1 - svm.score(train_x, train_y)\n",
    "        error_test[i][j] = 1 - svm.score(test_x, test_y)\n",
    "\n",
    "print(error_train)\n",
    "print(error_test)\n",
    "\n",
    "plt.imshow(error_train, cmap='gray', interpolation='none')\n",
    "plt.show()\n",
    "plt.imshow(error_test, cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On what basis would you decide that a hyperparameter setting is optimal? Which setting of (C, d) gave the optimal results?\n",
    "#### I chose to define optimal as the settings that produced the least error in both test and training sets. In this case, the optimal setting was (C = 1, d = [1, 2, 3, or 5])\n",
    "\n",
    "### Youâ€™ll notice that between C and d, one factor mattered far more than the other. What can you conclude about the structure of the datasets and how they were generated?\n",
    "#### It appears that C was far more important for this data because the real change in the error rates came after a change in C from 0.1 to 1. This suggests that there are not many misclassified points in our model since the penalty that resulted with the lowest error was C=1. This could mean that the data has a smaller margin between the linear separator and the points are packed close together. Also shown in the data, changing the degree of the polynomial did not affect the results which means that the data is constructed to fit these degree shapes.\n",
    "\n",
    "### With as much granularity as possible, which hyperparameter settings are underfitting and which are overfitting? What allows you to make this claim?\n",
    "#### The lower values of C's are underfitting while d is overfitting. This is because the value of C with the lowest error is the default 1. Also, the value of d did not have any effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma:  0.001\n",
      "scores:  [Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.085'), Decimal('0.085')]\n",
      "gamma:  0.01\n",
      "scores:  [Decimal('0.042'), Decimal('0.070'), Decimal('0.049'), Decimal('0.042'), Decimal('0.056'), Decimal('0.035'), Decimal('0.035')]\n",
      "gamma:  0.1\n",
      "scores:  [Decimal('0.035'), Decimal('0.056'), Decimal('0.007'), Decimal('0.049'), Decimal('0.035'), Decimal('0.021'), Decimal('0.028')]\n",
      "gamma:  1\n",
      "scores:  [Decimal('0.049'), Decimal('0.063'), Decimal('0.035'), Decimal('0.063'), Decimal('0.042'), Decimal('0.042'), Decimal('0.028')]\n",
      "gamma:  10\n",
      "scores:  [Decimal('0.091'), Decimal('0.091'), Decimal('0.084'), Decimal('0.091'), Decimal('0.091'), Decimal('0.077'), Decimal('0.077')]\n",
      "gamma:  100\n",
      "scores:  [Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.085'), Decimal('0.085')]\n",
      "gamma:  1000\n",
      "scores:  [Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.091'), Decimal('0.085'), Decimal('0.085')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gammas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "results = []\n",
    "train_y = np.ravel(train_y)\n",
    "for gamma in gammas:\n",
    "    svm = SVC(kernel='rbf', gamma=gamma)\n",
    "    scores = cross_val_score(svm, train_x, train_y, cv=7)\n",
    "    scores\n",
    "    results.append([round(1 - score, 3) for score in scores])\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    print(\"gamma: \", gammas[i])\n",
    "    print(\"scores: \", results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
